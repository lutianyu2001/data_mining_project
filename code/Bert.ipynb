{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eee29952",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b059f5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://minio.cvmart.net/user-file/23708/601a4f6ce5784700850b859d366b7a26.csv\"\n",
    "# urllib.request.urlretrieve(url, \"train_data_noWordCut.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4f8d5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_all = pd.read_csv(\"train_data_noWordCut.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d38e2a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_all = pd.DataFrame(pd_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d860d943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{%#静下心来听音乐#%} “书中自有黄金屋，书中自有颜如玉”。沿着岁月的长河跋涉，或是风光...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>这是英超被黑的最惨的一次[二哈][二哈]十几年来，中国只有孙继海，董方卓，郑智，李铁登陆过英...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>【俞曾港：专业聚焦和产业链延伸是企业“走出去”根本要义】中国远洋海运集团副总经理俞曾港4月2...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>看《流星花园》其实也还好啦，现在的观念以及时尚眼光都不一样了，或许十几年之后的人看我们的现在...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>汉武帝的罪己诏的真实性尽管存在着争议，然而“轮台罪己诏”作为中国历史上第一份皇帝自我批评的文...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>我努力的等着我想要的结局。但在某个时刻，我感觉到似乎早已有了结局。不知是我没有感觉到，还是感...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>儿时的光阴永远是最美好的，从你还在襁褓中开始，一路陪伴我做琴童的日子，之后你也不可豁免滴成了...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{%#岳岳pinkray#%} 🌙{%#ONER#%} 180421  pek再看我一眼吧再...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>流浪狗小黑每天坚持翻墙“非法入侵”民宅，只为了见好朋友Sooni一面，狗狗的执着打动了屋主，...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>我单方面宣布：黑色和粉色是全世界最浪漫的配色 ​</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  {%#静下心来听音乐#%} “书中自有黄金屋，书中自有颜如玉”。沿着岁月的长河跋涉，或是风光...      1\n",
       "1  这是英超被黑的最惨的一次[二哈][二哈]十几年来，中国只有孙继海，董方卓，郑智，李铁登陆过英...      0\n",
       "2  【俞曾港：专业聚焦和产业链延伸是企业“走出去”根本要义】中国远洋海运集团副总经理俞曾港4月2...      1\n",
       "3  看《流星花园》其实也还好啦，现在的观念以及时尚眼光都不一样了，或许十几年之后的人看我们的现在...      1\n",
       "4  汉武帝的罪己诏的真实性尽管存在着争议，然而“轮台罪己诏”作为中国历史上第一份皇帝自我批评的文...      1\n",
       "5  我努力的等着我想要的结局。但在某个时刻，我感觉到似乎早已有了结局。不知是我没有感觉到，还是感...      0\n",
       "6  儿时的光阴永远是最美好的，从你还在襁褓中开始，一路陪伴我做琴童的日子，之后你也不可豁免滴成了...      1\n",
       "7  {%#岳岳pinkray#%} 🌙{%#ONER#%} 180421  pek再看我一眼吧再...      1\n",
       "8  流浪狗小黑每天坚持翻墙“非法入侵”民宅，只为了见好朋友Sooni一面，狗狗的执着打动了屋主，...      1\n",
       "9                           我单方面宣布：黑色和粉色是全世界最浪漫的配色 ​      1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_all.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f33a403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://minio.cvmart.net/user-file/23708/65afee6aa9a04bb180c2cd79ef773e44.csv\"\n",
    "# urllib.request.urlretrieve(url, \"predict_data_noWordCut.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "243af054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = \"cpu\" # not using CUDA as we do not have CUDA (AMD GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71f628fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "learning_rate = 1e-3\n",
    "input_size = 768\n",
    "num_epoches = 10\n",
    "batch_size = 100\n",
    "decay_rate = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "740094a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the uploaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aafb011d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://minio.cvmart.net/user-file/23708/473d59a6c58d4647b9c7a9ce945f7f95.txt\"\n",
    "# urllib.request.urlretrieve(url, \"vocab.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c427b64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://minio.cvmart.net/user-file/23708/ad041a10c24c47a6818f4c2379704d69.bin\"\n",
    "# urllib.request.urlretrieve(url, \"pytorch_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b6ca979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://minio.cvmart.net/user-file/23708/d9e7354a821a49a2a1e0e3762c2c71df.json\"\n",
    "# urllib.request.urlretrieve(url, \"config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26304c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "MODEL_PATH = \"./chinese_wwm_pytorch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4b23635",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./chinese_wwm_pytorch were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)\n",
    "bert = BertModel.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e7a9a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.data = pd_all[\"text\"].tolist()\n",
    "        self.label = pd_all[\"label\"].tolist()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index]\n",
    "        label = self.label[index]\n",
    "        return data, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "# training set\n",
    "train_data = MyDataset(pd_all)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# testing set\n",
    "test_data = MyDataset(pd_all)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66dfef6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network structure\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc(x)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "net = Net(input_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f8a4667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set effect test\n",
    "def test():\n",
    "    y_pred, y_true = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for words, labels in test_loader:\n",
    "            tokens = tokenizer(words, padding=True)\n",
    "            input_ids = torch.tensor(tokens[\"input_ids\"]).to(device)\n",
    "            attention_mask = torch.tensor(tokens[\"attention_mask\"]).to(device)\n",
    "            last_hidden_states = bert(input_ids, attention_mask=attention_mask)\n",
    "            bert_output = last_hidden_states[0][:, 0]\n",
    "            outputs = net(bert_output)          # Forward propagation\n",
    "            outputs = outputs.view(-1)          # Flattening the output\n",
    "            y_pred.append(outputs)\n",
    "            y_true.append(labels)\n",
    "\n",
    "    y_prob = torch.cat(y_pred)\n",
    "    y_true = torch.cat(y_true)\n",
    "    y_pred = y_prob.clone()\n",
    "    y_pred[y_pred > 0.5] = 1\n",
    "    y_pred[y_pred <= 0.5] = 0\n",
    "    \n",
    "    print(metrics.classification_report(y_true, y_pred))\n",
    "    print(\"准确率:\", metrics.accuracy_score(y_true, y_pred))\n",
    "    print(\"AUC:\", metrics.roc_auc_score(y_true, y_prob) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b1084cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining loss functions and optimisers\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=decay_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9f3363e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterative training\n",
    "# for epoch in range(num_epoches):\n",
    "#     total_loss = 0\n",
    "#     for i, (words, labels) in enumerate(train_loader):\n",
    "#         tokens = tokenizer(words, padding=True)\n",
    "#         input_ids = torch.tensor(tokens[\"input_ids\"]).to(device)\n",
    "#         attention_mask = torch.tensor(tokens[\"attention_mask\"]).to(device)\n",
    "#         labels = labels.float().to(device)\n",
    "#         with torch.no_grad():\n",
    "#             last_hidden_states = bert(input_ids, attention_mask=attention_mask)\n",
    "#             bert_output = last_hidden_states[0][:, 0]\n",
    "#         optimizer.zero_grad()               # 梯度清零\n",
    "#         outputs = net(bert_output)          # 前向传播\n",
    "#         logits = outputs.view(-1)           # 将输出展平\n",
    "#         loss = criterion(logits, labels)    # loss计算\n",
    "#         total_loss += loss\n",
    "#         loss.backward()                     # 反向传播，计算梯度\n",
    "#         optimizer.step()                    # 梯度更新\n",
    "#         if (i+1) % 10 == 0:\n",
    "#             print(\"epoch:{}, step:{}, loss:{}\".format(epoch+1, i+1, total_loss/10))\n",
    "#             total_loss = 0\n",
    "    \n",
    "#     # learning_rate decay\n",
    "#     scheduler.step()\n",
    "    \n",
    "#     # test\n",
    "#     test()\n",
    "    \n",
    "#     # save model\n",
    "#     model_path = \"./model/bert_dnn_{}.model\".format(epoch+1)\n",
    "#     torch.save(net, model_path)\n",
    "#     print(\"saved model: \", model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "793dca91",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.load(\"./model/bert_dnn_3.model\")    # Use the best model we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6db5e63c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7710],\n",
       "        [0.1685]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = [\"华丽繁荣的城市、充满回忆的小镇、郁郁葱葱的山谷...\", \"突然就觉得人间不值得\"]\n",
    "tokens = tokenizer(s, padding=True)\n",
    "input_ids = torch.tensor(tokens[\"input_ids\"])\n",
    "attention_mask = torch.tensor(tokens[\"attention_mask\"])\n",
    "last_hidden_states = bert(input_ids, attention_mask=attention_mask)\n",
    "bert_output = last_hidden_states[0][:, 0]\n",
    "outputs = net(bert_output)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "07972d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9578],\n",
       "        [0.9773]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = [\"今天天气真好\", \"今天天气特别特别棒\"]\n",
    "tokens = tokenizer(s, padding=True)\n",
    "input_ids = torch.tensor(tokens[\"input_ids\"])\n",
    "attention_mask = torch.tensor(tokens[\"attention_mask\"])\n",
    "last_hidden_states = bert(input_ids, attention_mask=attention_mask)\n",
    "bert_output = last_hidden_states[0][:, 0]\n",
    "outputs = net(bert_output)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "abcc9e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def classify_All(dfRow):\n",
    "#     tokens = tokenizer(dfRow[\"text\"], padding=True)\n",
    "#     input_ids = torch.tensor(tokens[\"input_ids\"])\n",
    "#     attention_mask = torch.tensor(tokens[\"attention_mask\"])\n",
    "#     last_hidden_states = bert(input_ids, attention_mask=attention_mask)\n",
    "#     bert_output = last_hidden_states[0][:, 0]\n",
    "#     outputs = net(bert_output)\n",
    "#     return outputs\n",
    "\n",
    "# #dfVerify[\"bert_result\"] = dfVerify.apply(classify, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3fef93f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#classify_All(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "741755b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def classify_All(dfRow):\n",
    "#     tokens = tokenizer(dfRow[\"text\"], padding=True)\n",
    "#     input_ids = torch.tensor(tokens[\"input_ids\"])\n",
    "#     attention_mask = torch.tensor(tokens[\"attention_mask\"])\n",
    "#     last_hidden_states = bert(input_ids, attention_mask=attention_mask)\n",
    "#     bert_output = last_hidden_states[0][:, 0]\n",
    "#     outputs = net(bert_output)\n",
    "#     return outputs\n",
    "\n",
    "# dfPredict[\"bert_result\"] = dfPredict.apply(classify_All, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "297ecd51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('verify_data_noWordCut.csv', <http.client.HTTPMessage at 0x7fe2ef6949e8>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# url = \"https://minio.cvmart.net/user-file/23708/25224f724fdc4f08849ed69877aeab73.csv\"\n",
    "# urllib.request.urlretrieve(url, \"verify_data_noWordCut.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d1d5861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfVerify = pd.read_csv(\"verify_data_noWordCut.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5654fc25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>﻿更博了，爆照了，帅的呀，就是越来越爱你！生快傻缺[爱你][爱你][爱你]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>土耳其的事要认真对待[哈哈]，否则直接开除 很是细心，酒店都全部OK啦。</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>姑娘都羡慕你呢…还有招财猫高兴……[哈哈]小学徒一枚，等着明天见您呢大佬范儿[书呆子]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>美~~~~~[爱你]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>梦想有多大，舞台就有多大![鼓掌]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119983</th>\n",
       "      <td>一公里不到，县医院那个天桥下右拐200米就到了！ 我靠。这个太霸道了！离224有好远？ 这个...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119984</th>\n",
       "      <td>今天真冷啊，难道又要穿棉袄了[晕]？今年的春天真的是百变莫测啊[抓狂]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119985</th>\n",
       "      <td>最近几天就没停止过！！！[伤心]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119986</th>\n",
       "      <td>[怒] 很惨!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119987</th>\n",
       "      <td>呢? ？！[抓狂] ?搞乜鬼？？！！想知？入去GOtrip睇睇：</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>119988 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label\n",
       "0                   ﻿更博了，爆照了，帅的呀，就是越来越爱你！生快傻缺[爱你][爱你][爱你]      1\n",
       "1                    土耳其的事要认真对待[哈哈]，否则直接开除 很是细心，酒店都全部OK啦。      1\n",
       "2             姑娘都羡慕你呢…还有招财猫高兴……[哈哈]小学徒一枚，等着明天见您呢大佬范儿[书呆子]      1\n",
       "3                                              美~~~~~[爱你]      1\n",
       "4                                       梦想有多大，舞台就有多大![鼓掌]      1\n",
       "...                                                   ...    ...\n",
       "119983  一公里不到，县医院那个天桥下右拐200米就到了！ 我靠。这个太霸道了！离224有好远？ 这个...      0\n",
       "119984                今天真冷啊，难道又要穿棉袄了[晕]？今年的春天真的是百变莫测啊[抓狂]      0\n",
       "119985                                   最近几天就没停止过！！！[伤心]      0\n",
       "119986                                            [怒] 很惨!      0\n",
       "119987                 呢? ？！[抓狂] ?搞乜鬼？？！！想知？入去GOtrip睇睇：        0\n",
       "\n",
       "[119988 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display(dfVerify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b31281c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = tokenizer(dfVerify[\"text\"].tolist(), padding=True)\n",
    "# input_ids = torch.tensor(tokens[\"input_ids\"])\n",
    "# attention_mask = torch.tensor(tokens[\"attention_mask\"])\n",
    "# last_hidden_states = bert(input_ids, attention_mask=attention_mask)\n",
    "# bert_output = last_hidden_states[0][:, 0]\n",
    "# outputs = net(bert_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9f5126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def classify_All(dfRow):\n",
    "#     print(type(dfRow[\"text\"]))\n",
    "#     tokens = tokenizer(dfRow[\"text\"], padding=True)\n",
    "#     input_ids = tokens[\"input_ids\"]\n",
    "#     attention_mask = tokens[\"attention_mask\"]\n",
    "#     last_hidden_states = bert(input_ids, attention_mask=attention_mask)\n",
    "#     bert_output = last_hidden_states[0][:, 0]\n",
    "#     outputs = net(bert_output)\n",
    "#     return outputs\n",
    "\n",
    "# dfPredict[\"bert_result\"] = dfPredict.apply(lambda x: classify_All(x), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ecff12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfVerify[\"predict\"] = outputs.tolist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
