{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c36ca32",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'./'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "294fbddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import naive_bayes\n",
    "from sklearn import metrics\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e76877a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stopwords = set()\n",
    "\n",
    "# We revised the Baidu Stopwords and Sichuan University Stopwords to avoid filtering out the emotion words\n",
    "\n",
    "baidu_swFile = open(\"baidu_stopwords_revised.txt\", encoding = \"UTF-8\")\n",
    "for line in baidu_swFile:\n",
    "    all_stopwords.add(line.strip(\"\\n\"))\n",
    "    \n",
    "scu_swFile = open(\"scu_stopwords_revised.txt\", encoding = \"UTF-8\")\n",
    "for line in scu_swFile:\n",
    "    all_stopwords.add(line.strip(\"\\n\"))\n",
    "\n",
    "#print(all_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b94b0867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dumpModel(model, filename):\n",
    "    ''' Input Value\n",
    "            model:    the model you are going to dump\n",
    "            filename: the filename of the file to store the model, you are strongly suggested to use\n",
    "                      \".model\" as the suffix\n",
    "    '''\n",
    "    f = open(filename, 'wb')\n",
    "    pickle.dump(model, f)\n",
    "    \n",
    "def loadModel(filename):\n",
    "    ''' Input Value\n",
    "            filename: the filename of the file storing the model, you are strongly suggested to use\n",
    "                      \".model\" as the suffix\n",
    "    '''\n",
    "    f = open(filename, 'rb')\n",
    "    return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11b65753",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sentiment_XGB():\n",
    "    def __init__(self, data, stopwords):\n",
    "        ''' Input Value\n",
    "                data:      the dataset you are going to use to train the model, it will be split to \n",
    "                           two dataset, one is for training (dfTrain) and one is for testing (dfTest)\n",
    "                stopwords: the stopwords for Bag-of-Word / TF-IDF\n",
    "        '''\n",
    "        self.data = data\n",
    "        self.stopwords = stopwords\n",
    "        \n",
    "        self.vecr = None # the vectorizer for feature coding\n",
    "        self.featureMethod = \"undefined\" # which feature coding method are we using\n",
    "        \n",
    "        self.xgb_model = None # the best trained naive Bayes model\n",
    "        self.f1_score = 0  # the f1-score\n",
    "        self.classify_report = None # the deatailed classification report from sklearn\n",
    "        \n",
    "        self.bestCnt = 0 # the counter's value when we obtained the best model\n",
    "        self.trainCnt = 0 # the counter for training\n",
    "    \n",
    "    def classify(self, dfClassify):\n",
    "        ''' Input Value     \n",
    "                dfClassify: the dataframe includes the data we are going to classify\n",
    "            Return Value\n",
    "                resultDF:   the df based on dfClassify which shows the classification result\n",
    "                            on column \"predict\"\n",
    "        '''\n",
    "        \n",
    "        X = self.vecr.transform(dfClassify[\"text\"])\n",
    "        resultDF = dfClassify\n",
    "        \n",
    "        start = time.perf_counter()\n",
    "        dmatrix = xgb.DMatrix(X)\n",
    "        y = self.xgb_model.predict(dmatrix)\n",
    "        y[y >= 0.5] = 1\n",
    "        y[y < 0.5] = 0\n",
    "        #display(y_pred)\n",
    "        end = time.perf_counter()\n",
    "        timeCost = end - start\n",
    "        \n",
    "        print(\"\\nPredicting Finish Transcript\")\n",
    "        print(\"-----------------------------------------\")\n",
    "        print(\"Time Cost (Seconds): \",round(timeCost,8))\n",
    "        print(\"-----------------------------------------\")\n",
    "        \n",
    "        resultDF[\"predict\"] = y\n",
    "        return resultDF\n",
    "    \n",
    "    def training(self, iteration = 20, percent = 0.2, featureMethod = \"tfidf\", inplace = True):\n",
    "        ''' Input Value     \n",
    "                iteration:     the iteration round of training, default will be 5\n",
    "                percent:       the percentage of the testing set among the whole dataset, default will be 0.2\n",
    "                featureMethod: the method used in feature coding, can use \"tfidf\" or \"bow\",\n",
    "                               default will be \"tfidf\"\n",
    "                inplace:       whether to update the model if we have trained a better model, \n",
    "                               default will be True\n",
    "        '''\n",
    "        \n",
    "        bestModel = self.xgb_model\n",
    "        bestVecr = self.vecr\n",
    "        bestF1_score = self.f1_score\n",
    "        bestReport = self.classify_report\n",
    "        bestModelCnt = self.bestCnt\n",
    "        \n",
    "        print(\"Training Record\")\n",
    "        print(\"-------------------------------------------------------------------------\")\n",
    "        print(\"Total Training No. | Train Iter. No. |  f1_score  |   Time Cost (Seconds)\")\n",
    "        \n",
    "        for i in range(iteration):\n",
    "            dfTrain, dfTest = self.random_dataSplit(self.data, percent)\n",
    "            #print(dfTrain.shape[0], dfTest.shape[0])\n",
    "            \n",
    "            start = time.perf_counter()\n",
    "            vecr, xgb_model = self.singleTrain(dfTrain, featureMethod)\n",
    "            ac, report = self.__test(xgb_model, dfTest, vecr, featureMethod)\n",
    "            end = time.perf_counter()\n",
    "            \n",
    "            timeCost = end - start\n",
    "            self.trainCnt += 1\n",
    "            \n",
    "            print(\"\\t\",self.trainCnt,\"\\t\\t\",i+1,\"\\t      \",round(ac,8),\"\\t\",round(timeCost,8))\n",
    "            \n",
    "            if(ac > bestF1_score):\n",
    "                bestModel = xgb_model\n",
    "                bestVecr = vecr\n",
    "                bestF1_score = ac\n",
    "                bestReport = report\n",
    "                bestModelCnt = self.trainCnt\n",
    "        \n",
    "        print(\"\\nTraining Finish Transcript\")\n",
    "        print(\"-------------------------------------------------------------------------------\")\n",
    "        print(\"Best Model F1_score: \",bestF1_score,\"\\t\\t\",\"From Training No. : \",bestModelCnt)\n",
    "        print(\"-------------------------------------------------------------------------------\")\n",
    "        \n",
    "        if(inplace == True):\n",
    "            self.xgb_model = bestModel\n",
    "            self.vecr = bestVecr\n",
    "            self.featureMethod = featureMethod\n",
    "            self.f1_score = bestF1_score\n",
    "            self.classify_report = bestReport\n",
    "            self.bestCnt = bestModelCnt\n",
    "            return None\n",
    "        \n",
    "        return xgb_model, bestVecr, bestF1_score, bestReport\n",
    "        \n",
    "    def singleTrain(self, dataTrain, featureMethod = \"tfidf\"):\n",
    "        ''' Input Value     \n",
    "                dataTrain:     the dataset for training\n",
    "                featureMethod: the method used in feature coding, can use \"tfidf\" or \"bow\",\n",
    "                               default will be \"tfidf\"\n",
    "            Return Value\n",
    "                vecr:          the trained vectorizer\n",
    "                xgb_model: the trained naive Bayes model\n",
    "        '''\n",
    "        if(featureMethod == \"bow\"):\n",
    "            X_train, y_train, vecr = self.BOWcoding(dataTrain, 0)\n",
    "        else:\n",
    "            X_train, y_train, vecr = self.TFIDFcoding(dataTrain, 0)\n",
    "        \n",
    "        param = {\n",
    "            'booster':'gbtree',\n",
    "            'max_depth': 6, \n",
    "            'scale_pos_weight': 0.5,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'objective': 'binary:logistic',\n",
    "            'eval_metric': 'error',\n",
    "            'eta': 0.3,\n",
    "            'nthread': 10,\n",
    "        }\n",
    "        dmatrix = xgb.DMatrix(X_train, label = y_train)\n",
    "        xgb_model = xgb.train(param, dmatrix, num_boost_round = 200)\n",
    "        \n",
    "        return vecr, xgb_model\n",
    "\n",
    "    def random_dataSplit(self, dataTrain, percent = 0.2):\n",
    "        ''' Input Value\n",
    "                dataTrain: the dataset you are going to use to train the model, it will be split to \n",
    "                           two dataset, one is for training (dfTrain) and one is for testing (dfTest)\n",
    "                percent:   the percentage of the testing set among the whole dataset, default will be 0.2\n",
    "            Return Value\n",
    "                dfTrain:   splited dataset for training\n",
    "                dfTest:    splited dataset for testing\n",
    "        '''\n",
    "        \n",
    "        # Split the data into two parts according to the category, \n",
    "        # And split the two parts into training set and testing set according to \"percent\"\n",
    "        # Then combine the training set together and the testing set together \n",
    "        # to ensure the training set and testing set has a balanced proportion of categories\n",
    "        \n",
    "        dataPos = dataTrain.loc[dataTrain[\"label\"] == 1]\n",
    "        dataNeg = dataTrain.loc[dataTrain[\"label\"] == 0]\n",
    "        shuffleIdxPos = np.random.permutation(dataPos.shape[0])\n",
    "        shuffleIdxNeg = np.random.permutation(dataNeg.shape[0])\n",
    "        \n",
    "        dataPosTestSize = int(dataPos.shape[0]*percent)\n",
    "        dataNegTestSize = int(dataNeg.shape[0]*percent)\n",
    "        \n",
    "        dfTrainPos = dataPos.iloc[shuffleIdxPos[dataPosTestSize:]]\n",
    "        dfTestPos = dataPos.iloc[shuffleIdxPos[:dataPosTestSize]]\n",
    "        \n",
    "        dfTrainNeg = dataNeg.iloc[shuffleIdxNeg[dataNegTestSize:]]\n",
    "        dfTestNeg = dataNeg.iloc[shuffleIdxNeg[:dataNegTestSize]]\n",
    "        \n",
    "        \n",
    "        dfTrain = pd.concat([dfTrainPos, dfTrainNeg]).reset_index(drop = True)\n",
    "        dfTest  = pd.concat([dfTestPos, dfTestNeg]).reset_index(drop = True)\n",
    "        \n",
    "        return dfTrain, dfTest\n",
    "    \n",
    "    def TFIDFcoding(self, data, dataType, tfidf_vecr = None):\n",
    "        ''' Input Value\n",
    "                data:       the data you are going to apply TF-IDF\n",
    "                dataType:   whether the data is a training set or a testing set\n",
    "                            dataType == 0: training set\n",
    "                            dataType == 1: testing set\n",
    "                tfidf_vecr: the trained TfidfVectorizer, only needed when dataType == 1\n",
    "            Return Value\n",
    "                X:          the attribute columns\n",
    "                y:          the label column\n",
    "                tfidf_vecr: the trained TfidfVectorizer, only return when dataType == 0\n",
    "        '''\n",
    "        # TF-IDF feature coding\n",
    "        if(dataType == 0):\n",
    "            tfidf_vecr = TfidfVectorizer(token_pattern = '\\[?\\w+\\]?', stop_words = self.stopwords)\n",
    "            X = tfidf_vecr.fit_transform(data[\"text\"])\n",
    "            y = data[\"label\"]\n",
    "            return X, y, tfidf_vecr\n",
    "        elif(dataType == 1):\n",
    "            X = tfidf_vecr.transform(data[\"text\"])\n",
    "            y = data[\"label\"]\n",
    "            return X, y\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def BOWcoding(self, data, dataType, bow_vecr = None):\n",
    "        ''' Input Value\n",
    "                data:     the data you are going to apply BOW\n",
    "                dataType: whether the data is a training set or a testing set\n",
    "                          dataType == 0: training set\n",
    "                          dataType == 1: testing set\n",
    "                bow_vecr: the trained CountVectorizer, only needed when dataType == 1\n",
    "            Return Value\n",
    "                X:        the attribute columns\n",
    "                y:        the label column\n",
    "                bow_vecr: the trained CountVectorizer, only return when dataType == 0\n",
    "        '''\n",
    "        # Bag-of-Word feature coding\n",
    "        if(dataType == 0):\n",
    "            bow_vecr = CountVectorizer(token_pattern='\\[?\\w+\\]?', stop_words = self.stopwords)\n",
    "            X = bow_vecr.fit_transform(data[\"text\"])\n",
    "            y = data[\"label\"]\n",
    "            return X, y, bow_vecr\n",
    "        elif(dataType == 1):\n",
    "            X = bow_vecr.transform(data[\"text\"])\n",
    "            y = data[\"label\"]\n",
    "            return X, y\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def __test(self, xgb_model, dataTest, vecr, featureMethod = \"tfidf\"):\n",
    "        ''' Introduction\n",
    "                The private method for testing the trained model for selecting the best model by f1score\n",
    "            Input Value     \n",
    "                xgb_model:   the naive Bayes model we obtained from singleTrain()\n",
    "                dataTest:      the testing set\n",
    "                vecr:          the trained vectorizer\n",
    "                featureMethod: the method used in feature coding, can use \"tfidf\" or \"bow\",\n",
    "                               default will be \"tfidf\"\n",
    "            Return Value\n",
    "                test_f1score: the f1score\n",
    "                test_report:  the classification report from sklearn\n",
    "        '''\n",
    "        if(self.featureMethod == \"bow\"):\n",
    "            X_test, y_test = self.BOWcoding(dataTest, 1, vecr)\n",
    "        else:\n",
    "            X_test, y_test = self.TFIDFcoding(dataTest, 1, vecr)\n",
    "        \n",
    "        dmatrix = xgb.DMatrix(X_test)\n",
    "        y_pred = xgb_model.predict(dmatrix)\n",
    "        y_pred[y_pred >= 0.5] = 1\n",
    "        y_pred[y_pred < 0.5] = 0\n",
    "        #display(y_pred)\n",
    "        \n",
    "        test_f1score = metrics.f1_score(y_test, y_pred, average = \"micro\")\n",
    "        test_report = metrics.classification_report(y_test, y_pred)\n",
    "        \n",
    "        return test_f1score, test_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3de16a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path+\"train_data_wordCut.csv\")\n",
    "XGB = sentiment_XGB(df, all_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2bf70eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Record\n",
      "-------------------------------------------------------------------------\n",
      "Total Training No. | Train Iter. No. |  f1_score  |   Time Cost (Seconds)\n",
      "\t 1 \t\t 1 \t       0.82341171 \t 5.20033998\n",
      "\t 2 \t\t 2 \t       0.82841421 \t 6.32952548\n",
      "\t 3 \t\t 3 \t       0.82841421 \t 7.27272253\n",
      "\t 4 \t\t 4 \t       0.82291146 \t 7.60513378\n",
      "\t 5 \t\t 5 \t       0.8174087 \t 7.16716193\n",
      "\n",
      "Training Finish Transcript\n",
      "-------------------------------------------------------------------------------\n",
      "Best Model F1_score:  0.8284142071035517 \t\t From Training No. :  2\n",
      "-------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "XGB.training(iteration = 5, featureMethod = \"bow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "faa4e65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Record\n",
      "-------------------------------------------------------------------------\n",
      "Total Training No. | Train Iter. No. |  f1_score  |   Time Cost (Seconds)\n",
      "\t 6 \t\t 1 \t       0.7953977 \t 11.62900703\n",
      "\t 7 \t\t 2 \t       0.79989995 \t 12.88836594\n",
      "\t 8 \t\t 3 \t       0.81490745 \t 12.45530359\n",
      "\t 9 \t\t 4 \t       0.8084042 \t 12.69905558\n",
      "\t 10 \t\t 5 \t       0.80590295 \t 16.38711024\n",
      "\n",
      "Training Finish Transcript\n",
      "-------------------------------------------------------------------------------\n",
      "Best Model F1_score:  0.8284142071035517 \t\t From Training No. :  2\n",
      "-------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "XGB.training(iteration = 5, featureMethod = \"tfidf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "352e39e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Record\n",
      "-------------------------------------------------------------------------\n",
      "Total Training No. | Train Iter. No. |  f1_score  |   Time Cost (Seconds)\n",
      "\t 11 \t\t 1 \t       0.82741371 \t 5.74930577\n",
      "\t 12 \t\t 2 \t       0.82241121 \t 6.42344167\n",
      "\t 13 \t\t 3 \t       0.81490745 \t 8.43734376\n",
      "\t 14 \t\t 4 \t       0.82141071 \t 7.8077566\n",
      "\t 15 \t\t 5 \t       0.81790895 \t 7.73686426\n",
      "\t 16 \t\t 6 \t       0.82241121 \t 7.30321976\n",
      "\t 17 \t\t 7 \t       0.82941471 \t 7.70376301\n",
      "\t 18 \t\t 8 \t       0.8104052 \t 8.08364601\n",
      "\t 19 \t\t 9 \t       0.8154077 \t 8.09688233\n",
      "\t 20 \t\t 10 \t       0.80890445 \t 7.8309903\n",
      "\t 21 \t\t 11 \t       0.8184092 \t 8.00135365\n",
      "\t 22 \t\t 12 \t       0.82191096 \t 9.41382562\n",
      "\t 23 \t\t 13 \t       0.82191096 \t 8.33957885\n",
      "\t 24 \t\t 14 \t       0.82641321 \t 8.26821281\n",
      "\t 25 \t\t 15 \t       0.82141071 \t 8.20807695\n",
      "\t 26 \t\t 16 \t       0.82841421 \t 8.00696177\n",
      "\t 27 \t\t 17 \t       0.8154077 \t 8.05359279\n",
      "\t 28 \t\t 18 \t       0.82491246 \t 7.98776789\n",
      "\t 29 \t\t 19 \t       0.81190595 \t 8.08861135\n",
      "\t 30 \t\t 20 \t       0.82441221 \t 8.08171555\n",
      "\t 31 \t\t 21 \t       0.82541271 \t 8.10039413\n",
      "\t 32 \t\t 22 \t       0.84392196 \t 7.66825976\n",
      "\t 33 \t\t 23 \t       0.81390695 \t 8.35286317\n",
      "\t 34 \t\t 24 \t       0.80090045 \t 7.80584612\n",
      "\t 35 \t\t 25 \t       0.82191096 \t 7.95454805\n",
      "\t 36 \t\t 26 \t       0.8144072 \t 8.03038005\n",
      "\t 37 \t\t 27 \t       0.8114057 \t 8.53818784\n",
      "\t 38 \t\t 28 \t       0.82441221 \t 7.93689533\n",
      "\t 39 \t\t 29 \t       0.80990495 \t 7.51694331\n",
      "\t 40 \t\t 30 \t       0.82341171 \t 7.94979178\n",
      "\t 41 \t\t 31 \t       0.83041521 \t 8.10209281\n",
      "\t 42 \t\t 32 \t       0.8164082 \t 7.81524744\n",
      "\t 43 \t\t 33 \t       0.81990995 \t 8.06361995\n",
      "\t 44 \t\t 34 \t       0.82341171 \t 7.98315921\n",
      "\t 45 \t\t 35 \t       0.80890445 \t 8.17763793\n",
      "\t 46 \t\t 36 \t       0.82191096 \t 8.07903639\n",
      "\t 47 \t\t 37 \t       0.82941471 \t 7.83332164\n",
      "\t 48 \t\t 38 \t       0.83641821 \t 7.75774085\n",
      "\t 49 \t\t 39 \t       0.8154077 \t 7.94729506\n",
      "\t 50 \t\t 40 \t       0.8114057 \t 8.31297201\n",
      "\t 51 \t\t 41 \t       0.82491246 \t 7.8578896\n",
      "\t 52 \t\t 42 \t       0.80190095 \t 8.60233773\n",
      "\t 53 \t\t 43 \t       0.82891446 \t 7.89369164\n",
      "\t 54 \t\t 44 \t       0.82291146 \t 7.89142936\n",
      "\t 55 \t\t 45 \t       0.82241121 \t 8.0064014\n",
      "\t 56 \t\t 46 \t       0.82341171 \t 8.36495724\n",
      "\t 57 \t\t 47 \t       0.83441721 \t 7.70331119\n",
      "\t 58 \t\t 48 \t       0.82891446 \t 8.10262505\n",
      "\t 59 \t\t 49 \t       0.82541271 \t 8.65409883\n",
      "\t 60 \t\t 50 \t       0.8114057 \t 7.70558672\n",
      "\n",
      "Training Finish Transcript\n",
      "-------------------------------------------------------------------------------\n",
      "Best Model F1_score:  0.8439219609804902 \t\t From Training No. :  32\n",
      "-------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Clearly that the XGBoost with Bag-of-Word has the best performance\n",
    "# Therefore we use XGBoost with Bag-of-Word as our model for prediction\n",
    "# And now we are going to train a best model\n",
    "XGB.training(iteration = 50, featureMethod = \"bow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8ec14fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>﻿ 更博 了 ， 爆照 了 ， 帅 的 呀 ， 就是 越来越 爱 你 ！ 生快 傻 缺 [ ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>土耳其 的 事要 认真对待 [ 哈哈 ] ， 否则 直接 开除   很 是 细心 ， 酒...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>姑娘 都 羡慕 你 呢 … 还有 招财猫 高兴 … … [ 哈哈 ] 小 学徒 一枚 ， 等...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>美 ~ ~ ~ ~ ~ [ 爱 你 ]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>梦想 有 多 大 ， 舞台 就 有 多 大 ! [ 鼓掌 ]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119983</th>\n",
       "      <td>一 公里 不到 ， 县 医院 那个 天桥 下右 拐 200 米 就 到 了 ！   我 靠 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119984</th>\n",
       "      <td>今天 真冷 啊 ， 难道 又 要 穿 棉袄 了 [ 晕 ] ？ 今年 的 春天 真的 是 百...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119985</th>\n",
       "      <td>最近 几天 就 没 停止 过 ！ ！ ！ [ 伤心 ]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119986</th>\n",
       "      <td>[ 怒 ]   很惨 !</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119987</th>\n",
       "      <td>呢 ?   ？ ！ [ 抓狂 ]   ? 搞 乜 鬼 ？ ？ ！ ！ 想知 ？ 入去 GOt...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>119988 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label\n",
       "0       ﻿ 更博 了 ， 爆照 了 ， 帅 的 呀 ， 就是 越来越 爱 你 ！ 生快 傻 缺 [ ...      1\n",
       "1         土耳其 的 事要 认真对待 [ 哈哈 ] ， 否则 直接 开除   很 是 细心 ， 酒...      1\n",
       "2       姑娘 都 羡慕 你 呢 … 还有 招财猫 高兴 … … [ 哈哈 ] 小 学徒 一枚 ， 等...      1\n",
       "3                                     美 ~ ~ ~ ~ ~ [ 爱 你 ]      1\n",
       "4                          梦想 有 多 大 ， 舞台 就 有 多 大 ! [ 鼓掌 ]      1\n",
       "...                                                   ...    ...\n",
       "119983  一 公里 不到 ， 县 医院 那个 天桥 下右 拐 200 米 就 到 了 ！   我 靠 ...      0\n",
       "119984  今天 真冷 啊 ， 难道 又 要 穿 棉袄 了 [ 晕 ] ？ 今年 的 春天 真的 是 百...      0\n",
       "119985                        最近 几天 就 没 停止 过 ！ ！ ！ [ 伤心 ]      0\n",
       "119986                                       [ 怒 ]   很惨 !      0\n",
       "119987  呢 ?   ？ ！ [ 抓狂 ]   ? 搞 乜 鬼 ？ ？ ！ ！ 想知 ？ 入去 GOt...      0\n",
       "\n",
       "[119988 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_verify = pd.read_csv(path+\"verify_data_wordCut.csv\")\n",
    "display(df_verify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3543cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting Finish Transcript\n",
      "-----------------------------------------\n",
      "Time Cost (Seconds):  0.28205458\n",
      "-----------------------------------------\n",
      "0.8215404873820715\n"
     ]
    }
   ],
   "source": [
    "df_verify_predicted = XGB.classify(df_verify)\n",
    "y_veri = df_verify_predicted[\"label\"]\n",
    "y_pred = df_verify_predicted[\"predict\"]\n",
    "test_f1score = metrics.f1_score(y_veri, y_pred, average = \"micro\")\n",
    "print(test_f1score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c9a4150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the verified result for Bagging\n",
    "df_verified = pd.DataFrame()\n",
    "df_verified[\"label\"] = y_veri\n",
    "df_verified[\"predict\"] = y_pred\n",
    "df_verified.to_csv(\"verified_data_XGB.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "75e5c789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backup the trained model here\n",
    "dumpModel(bayes, \"XGB_trained.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66d0185c",
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB_trained = loadModel(\"XGB_trained.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9107abc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict = pd.read_csv(path+\"predict_data_wordCut.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "23e3e930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>哈哈哈哈 哈哈哈 确实 是 ， 我们 有 宝藏 他们 没有 ！</td>\n",
       "      <td>2020-03-12 01:41:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>（ 二 ） 真诚 的 认为 遇上 你 是 我 的 缘</td>\n",
       "      <td>2020-03-12 01:41:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td># 欧洲 求助 钟南山 # 人 都 是 思变 的 动物 ， 天仙 美眷 女人 也 留不住 完...</td>\n",
       "      <td>2020-03-12 01:41:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>钟老 ， 您 就是 夜空 中 最亮 的 星</td>\n",
       "      <td>2020-03-12 01:41:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>两人 目光 呆滞 ， 钟南山 全程 英语 分享 中国 经验 走路 脚跟 离 地 ， 九叔 一...</td>\n",
       "      <td>2020-03-12 01:40:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3112102</th>\n",
       "      <td>【 港新网 / 哈哈 ！ 黄之锋 动用 关系 从 美国 搞来 了 一批 “ 中国 制造 ” ...</td>\n",
       "      <td>2020-02-13 16:58:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3112103</th>\n",
       "      <td>上海 居委会 预约 的 口罩   质量 还 不错 哦   四毛 六 一个</td>\n",
       "      <td>2020-02-13 16:58:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3112104</th>\n",
       "      <td> 李现 # 李现 教 你 如何 节约 口罩 #   lx # 李现 公益 正 能量 #  ...</td>\n",
       "      <td>2020-02-13 16:58:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3112105</th>\n",
       "      <td>一次性 口罩 消毒 后 可以 反复 使用 ！ 您 知道 吗 ？   2 长沙 · 长沙县</td>\n",
       "      <td>2020-02-13 16:58:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3112106</th>\n",
       "      <td># 战疫 打卡 行动 # 我 已 打卡 2 天 ， 面对 疫情 我 承诺 ： 出门 戴 口罩...</td>\n",
       "      <td>2020-02-13 16:58:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3112107 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text  \\\n",
       "0                          哈哈哈哈 哈哈哈 确实 是 ， 我们 有 宝藏 他们 没有 ！   \n",
       "1                               （ 二 ） 真诚 的 认为 遇上 你 是 我 的 缘   \n",
       "2        # 欧洲 求助 钟南山 # 人 都 是 思变 的 动物 ， 天仙 美眷 女人 也 留不住 完...   \n",
       "3                                    钟老 ， 您 就是 夜空 中 最亮 的 星   \n",
       "4        两人 目光 呆滞 ， 钟南山 全程 英语 分享 中国 经验 走路 脚跟 离 地 ， 九叔 一...   \n",
       "...                                                    ...   \n",
       "3112102  【 港新网 / 哈哈 ！ 黄之锋 动用 关系 从 美国 搞来 了 一批 “ 中国 制造 ” ...   \n",
       "3112103               上海 居委会 预约 的 口罩   质量 还 不错 哦   四毛 六 一个   \n",
       "3112104   李现 # 李现 教 你 如何 节约 口罩 #   lx # 李现 公益 正 能量 #  ...   \n",
       "3112105       一次性 口罩 消毒 后 可以 反复 使用 ！ 您 知道 吗 ？   2 长沙 · 长沙县   \n",
       "3112106  # 战疫 打卡 行动 # 我 已 打卡 2 天 ， 面对 疫情 我 承诺 ： 出门 戴 口罩...   \n",
       "\n",
       "                   timestamp  \n",
       "0        2020-03-12 01:41:00  \n",
       "1        2020-03-12 01:41:00  \n",
       "2        2020-03-12 01:41:00  \n",
       "3        2020-03-12 01:41:00  \n",
       "4        2020-03-12 01:40:00  \n",
       "...                      ...  \n",
       "3112102  2020-02-13 16:58:00  \n",
       "3112103  2020-02-13 16:58:00  \n",
       "3112104  2020-02-13 16:58:00  \n",
       "3112105  2020-02-13 16:58:00  \n",
       "3112106  2020-02-13 16:58:00  \n",
       "\n",
       "[3112107 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d2b78109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting Finish Transcript\n",
      "-----------------------------------------\n",
      "Time Cost (Seconds):  9.90834145\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "df_predicted = XGB_trained.classify(df_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "848049e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>哈哈哈哈 哈哈哈 确实 是 ， 我们 有 宝藏 他们 没有 ！</td>\n",
       "      <td>2020-03-12 01:41:00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>（ 二 ） 真诚 的 认为 遇上 你 是 我 的 缘</td>\n",
       "      <td>2020-03-12 01:41:00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td># 欧洲 求助 钟南山 # 人 都 是 思变 的 动物 ， 天仙 美眷 女人 也 留不住 完...</td>\n",
       "      <td>2020-03-12 01:41:00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>钟老 ， 您 就是 夜空 中 最亮 的 星</td>\n",
       "      <td>2020-03-12 01:41:00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>两人 目光 呆滞 ， 钟南山 全程 英语 分享 中国 经验 走路 脚跟 离 地 ， 九叔 一...</td>\n",
       "      <td>2020-03-12 01:40:00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3112102</th>\n",
       "      <td>【 港新网 / 哈哈 ！ 黄之锋 动用 关系 从 美国 搞来 了 一批 “ 中国 制造 ” ...</td>\n",
       "      <td>2020-02-13 16:58:00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3112103</th>\n",
       "      <td>上海 居委会 预约 的 口罩   质量 还 不错 哦   四毛 六 一个</td>\n",
       "      <td>2020-02-13 16:58:00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3112104</th>\n",
       "      <td> 李现 # 李现 教 你 如何 节约 口罩 #   lx # 李现 公益 正 能量 #  ...</td>\n",
       "      <td>2020-02-13 16:58:00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3112105</th>\n",
       "      <td>一次性 口罩 消毒 后 可以 反复 使用 ！ 您 知道 吗 ？   2 长沙 · 长沙县</td>\n",
       "      <td>2020-02-13 16:58:00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3112106</th>\n",
       "      <td># 战疫 打卡 行动 # 我 已 打卡 2 天 ， 面对 疫情 我 承诺 ： 出门 戴 口罩...</td>\n",
       "      <td>2020-02-13 16:58:00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3112107 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text  \\\n",
       "0                          哈哈哈哈 哈哈哈 确实 是 ， 我们 有 宝藏 他们 没有 ！   \n",
       "1                               （ 二 ） 真诚 的 认为 遇上 你 是 我 的 缘   \n",
       "2        # 欧洲 求助 钟南山 # 人 都 是 思变 的 动物 ， 天仙 美眷 女人 也 留不住 完...   \n",
       "3                                    钟老 ， 您 就是 夜空 中 最亮 的 星   \n",
       "4        两人 目光 呆滞 ， 钟南山 全程 英语 分享 中国 经验 走路 脚跟 离 地 ， 九叔 一...   \n",
       "...                                                    ...   \n",
       "3112102  【 港新网 / 哈哈 ！ 黄之锋 动用 关系 从 美国 搞来 了 一批 “ 中国 制造 ” ...   \n",
       "3112103               上海 居委会 预约 的 口罩   质量 还 不错 哦   四毛 六 一个   \n",
       "3112104   李现 # 李现 教 你 如何 节约 口罩 #   lx # 李现 公益 正 能量 #  ...   \n",
       "3112105       一次性 口罩 消毒 后 可以 反复 使用 ！ 您 知道 吗 ？   2 长沙 · 长沙县   \n",
       "3112106  # 战疫 打卡 行动 # 我 已 打卡 2 天 ， 面对 疫情 我 承诺 ： 出门 戴 口罩...   \n",
       "\n",
       "                   timestamp  predict  \n",
       "0        2020-03-12 01:41:00      1.0  \n",
       "1        2020-03-12 01:41:00      1.0  \n",
       "2        2020-03-12 01:41:00      1.0  \n",
       "3        2020-03-12 01:41:00      1.0  \n",
       "4        2020-03-12 01:40:00      1.0  \n",
       "...                      ...      ...  \n",
       "3112102  2020-02-13 16:58:00      1.0  \n",
       "3112103  2020-02-13 16:58:00      1.0  \n",
       "3112104  2020-02-13 16:58:00      1.0  \n",
       "3112105  2020-02-13 16:58:00      1.0  \n",
       "3112106  2020-02-13 16:58:00      1.0  \n",
       "\n",
       "[3112107 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "22a479d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predicted.to_csv(\"predict_data_XGB.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-default]",
   "language": "python",
   "name": "conda-env-.conda-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
